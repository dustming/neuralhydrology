"""
Routing Module for Neural Hydrology

This module implements various routing methods to route lateral flows from
individual basins (generated by LSTMs) to the watershed outlet.

The routing methods are inspired by the Raven hydrological modeling framework,
particularly the ROUTE_NONE method which provides instantaneous routing with
no travel time delays.
"""

import logging
from typing import Dict, Optional, List, Tuple
import pandas as pd
import torch
import torch.nn as nn
from pathlib import Path

from neuralhydrology.utils.config import Config

LOGGER = logging.getLogger(__name__)


class RoutingModule(nn.Module):
    """
    Main routing module that can support multiple routing methods.

    Currently implements ROUTE_NONE, but can be extended to support
    other routing methods like ROUTE_PLUG_FLOW, ROUTE_MUSKINGUM, etc.

    Parameters
    ----------
    cfg : Config
        Configuration object containing:
        - basin_ids: List[str] - Basin identifiers (order will be auto-corrected!)
        - routing_connectivity_file: str - Path to connectivity CSV file (required)
        - outlet_basin_id: str, optional - ID of watershed outlet basin (auto-detected if not provided)
        - device: torch.device - Computation device

    Important Notes
    ---------------
    Basin Ordering and Outlet Detection are Automatic:
    - Basin order is automatically determined from connectivity file
    - Outlet basin is automatically identified as the basin with no downstream connections
    - This mimics Raven's GetOrderedSubBasinIndex() functionality
    - Basins are always ordered from upstream to downstream
    - No need to worry about input basin order or outlet specification

    Example: Input basin_ids can be in any order like:
    ['outlet', 'middle', 'headwater_1', 'headwater_2']
    Will be automatically reordered to:
    ['headwater_1', 'headwater_2', 'middle', 'outlet']
    And 'outlet' will be automatically identified as the outlet basin.
    """

    def __init__(self, cfg: Config):
        super(RoutingModule, self).__init__()

        self.cfg = cfg
        self.routing_method = getattr(cfg, "routing_method", "route_none").lower()

        # Extract configuration parameters
        self.basin_ids = getattr(cfg, "basin_ids", None)
        if self.basin_ids is None:
            raise ValueError("Config must specify 'basin_ids'")
        self.num_basins = len(self.basin_ids)
        self.device = getattr(cfg, "device", torch.device("cpu"))

        # Load connectivity file - always required
        connectivity_file = getattr(cfg, "routing_connectivity_file", None)
        if connectivity_file is None:
            raise ValueError("Config must specify 'routing_connectivity_file'")

        connectivity_path = Path(connectivity_file)
        if not connectivity_path.exists():
            raise FileNotFoundError(f"Connectivity file {connectivity_file} not found")

        connectivity_df = pd.read_csv(connectivity_path)
        LOGGER.info(f"Loaded connectivity from {connectivity_file}")

        # Process connectivity to order basins, identify outlet, and build matrix
        self.basin_ids, outlet_basin_id, self.connectivity_matrix = (
            self._process_connectivity(connectivity_df)
        )
        LOGGER.info(f"Ordered basins based on connectivity: {self.basin_ids}")

        # Recreate basin mappings after reordering
        self.basin_to_idx = {
            basin_id: idx for idx, basin_id in enumerate(self.basin_ids)
        }

        # Set outlet index from automatically identified outlet
        self.outlet_idx = self.basin_to_idx[outlet_basin_id]

        # Validate connectivity matrix and basin ordering
        self._validate_connectivity_matrix()
        self._validate_basin_ordering(connectivity_df)

        # Register as buffer so it's moved with model
        self.register_buffer("routing_matrix", self.connectivity_matrix)

        LOGGER.info(f"RoutingModule initialized with {self.num_basins} basins")
        LOGGER.info(
            f"Outlet basin: {self.basin_ids[self.outlet_idx]} (index: {self.outlet_idx})"
        )

        # Initialize appropriate routing function based on method
        if self.routing_method == "route_none" or self.routing_method == "none":
            # Use simple ROUTE_NONE routing function
            self.route_flow = self._route_none
            LOGGER.info(
                "Using simple ROUTE_NONE routing (function-based implementation)"
            )
        elif (
            self.routing_method == "route_plug_flow"
            or self.routing_method == "plug_flow"
        ):
            # Use simple plug flow routing function (placeholder implementation)
            self.route_flow = self._route_plug_flow
            LOGGER.info("Using simple plug flow routing (placeholder implementation)")
        else:
            raise NotImplementedError(
                f"Routing method '{self.routing_method}' not implemented. "
                "Currently supported: 'route_none', 'route_plug_flow'."
            )

    def _route_plug_flow(
        self,
        channel_storage: torch.Tensor,
        upstream_inflow: torch.Tensor,
    ) -> torch.Tensor:
        """
        ROUTE_PLUG_FLOW channel outflow calculation (placeholder implementation).

        For ROUTE_PLUG_FLOW: would implement travel time delays and convolution routing.
        This is a placeholder that currently uses instantaneous routing like ROUTE_NONE.

        Parameters
        ----------
        channel_storage : torch.Tensor
            Current channel storage [batch_size, seq_len]
        upstream_inflow : torch.Tensor
            Upstream inflow [batch_size, seq_len]

        Returns
        -------
        torch.Tensor
            Channel outflow [batch_size, seq_len]
        """
        # Placeholder implementation - would implement plug flow with travel time delays
        # For now, just use instantaneous routing like ROUTE_NONE
        return self._route_none(channel_storage, upstream_inflow)

    def _route_none(
        self,
        channel_storage: torch.Tensor,
        upstream_inflow: torch.Tensor,
    ) -> torch.Tensor:
        """
        ROUTE_NONE channel outflow calculation.

        For ROUTE_NONE: instantaneous routing - channel outflow is simply
        the sum of channel storage and upstream inflow.

        Parameters
        ----------
        channel_storage : torch.Tensor
            Current channel storage [batch_size, seq_len]
        upstream_inflow : torch.Tensor
            Upstream inflow [batch_size, seq_len]

        Returns
        -------
        torch.Tensor
            Channel outflow [batch_size, seq_len]
        """
        # ROUTE_NONE: Instantaneous routing - outflow = storage + inflow
        return channel_storage + upstream_inflow

    def _process_connectivity(
        self, connectivity_df: pd.DataFrame
    ) -> Tuple[List[str], str, torch.Tensor]:
        """
        Process connectivity data to order basins, identify outlet, and build connectivity matrix.

        This combines basin ordering, outlet identification, and matrix construction in one
        efficient pass through the connectivity data. Mimics Raven's InitializeRoutingNetwork().

        Parameters
        ----------
        connectivity_df : pd.DataFrame
            DataFrame with connectivity information containing columns:
            ['upstream_basin_id', 'downstream_basin_id', 'weight']

        Returns
        -------
        Tuple[List[str], str, torch.Tensor]
            Tuple of (ordered basin IDs from upstream to downstream, outlet basin ID, connectivity matrix)
        """
        # Validate required columns
        required_cols = ["upstream_basin_id", "downstream_basin_id", "weight"]
        if not all(col in connectivity_df.columns for col in required_cols):
            raise ValueError(f"connectivity_df must have columns: {required_cols}")

        # Build a directed graph of basin connections
        from collections import defaultdict, deque

        # Create adjacency list and in-degree count
        graph = defaultdict(list)  # downstream connections
        in_degree = defaultdict(int)  # number of upstream basins
        out_degree = defaultdict(int)  # number of downstream basins
        all_basins = set()

        # Store connectivity data for matrix building
        connectivity_weights = {}  # (upstream, downstream) -> weight

        for _, row in connectivity_df.iterrows():
            upstream = str(row["upstream_basin_id"])
            downstream = str(row["downstream_basin_id"])
            weight = float(row["weight"])

            # Only consider basins that are in our basin_ids list
            if upstream in self.basin_ids and downstream in self.basin_ids:
                graph[upstream].append(downstream)
                in_degree[downstream] += 1
                out_degree[upstream] += 1
                all_basins.add(upstream)
                all_basins.add(downstream)

                # Store weight for matrix building
                connectivity_weights[(upstream, downstream)] = weight

        # Initialize in-degree for all basins (including those not in connectivity)
        for basin_id in self.basin_ids:
            if basin_id not in in_degree:
                in_degree[basin_id] = 0
            if basin_id not in out_degree:
                out_degree[basin_id] = 0
            all_basins.add(basin_id)

        # Find outlet basin (basin with no downstream connections)
        outlet_candidates = [basin for basin in all_basins if out_degree[basin] == 0]

        if len(outlet_candidates) == 0:
            raise ValueError(
                "No outlet basin found (all basins have downstream connections)"
            )
        elif len(outlet_candidates) > 1:
            LOGGER.warning(
                f"Multiple potential outlets found: {outlet_candidates}, using first one"
            )
            outlet_basin = outlet_candidates[0]
        else:
            outlet_basin = outlet_candidates[0]

        # Topological sort (Kahn's algorithm) - upstream to downstream
        ordered_basins = []
        queue = deque([basin for basin in all_basins if in_degree[basin] == 0])

        while queue:
            current = queue.popleft()
            ordered_basins.append(current)

            # Reduce in-degree of downstream basins
            for downstream in graph[current]:
                in_degree[downstream] -= 1
                if in_degree[downstream] == 0:
                    queue.append(downstream)

        # Check if we have all basins (detect cycles)
        if len(ordered_basins) != len(self.basin_ids):
            missing_basins = set(self.basin_ids) - set(ordered_basins)
            LOGGER.warning(
                f"Could not determine order for basins (possible cycles): {missing_basins}"
            )
            # Add missing basins at the end
            ordered_basins.extend(missing_basins)

        # Build connectivity matrix using ordered basins
        basin_to_idx = {basin_id: idx for idx, basin_id in enumerate(ordered_basins)}
        num_basins = len(ordered_basins)
        matrix = torch.zeros(num_basins, num_basins, device=self.device)

        # Populate matrix with connectivity weights
        for (upstream, downstream), weight in connectivity_weights.items():
            upstream_idx = basin_to_idx[upstream]
            downstream_idx = basin_to_idx[downstream]
            matrix[upstream_idx, downstream_idx] = weight

        LOGGER.info(f"Basin routing order determined: {ordered_basins}")
        LOGGER.info(f"Outlet basin identified from connectivity: {outlet_basin}")
        return ordered_basins, outlet_basin, matrix

    def _validate_connectivity_matrix(self):
        """Validate the connectivity matrix."""
        if self.connectivity_matrix.shape != (self.num_basins, self.num_basins):
            raise ValueError(
                f"Connectivity matrix shape {self.connectivity_matrix.shape} "
                f"doesn't match number of basins {self.num_basins}"
            )

        # Check that each row sums to <= 1.0 (allowing for losses)
        row_sums = self.connectivity_matrix.sum(dim=1)
        if torch.any(row_sums > 1.01):  # Small tolerance for numerical errors
            problematic_basins = [
                self.basin_ids[i] for i in torch.where(row_sums > 1.01)[0]
            ]
            raise ValueError(
                f"Connectivity matrix row sums exceed 1.0 for basins: {problematic_basins}"
            )

        # Outlet should not flow to other basins (except itself)
        outlet_outflows = self.connectivity_matrix[self.outlet_idx, :].clone()
        outlet_outflows[self.outlet_idx] = 0  # Exclude self-flow
        if torch.any(outlet_outflows > 1e-6):
            LOGGER.warning(
                f"Outlet basin {self.basin_ids[self.outlet_idx]} has outflows to other basins"
            )

    def _validate_basin_ordering(self, connectivity_df: pd.DataFrame):
        """
        Validate that basins are ordered correctly (upstream to downstream).

        Since basins are automatically reordered from connectivity, this should
        always pass, but we keep it as a sanity check.
        """
        violations = []

        for _, row in connectivity_df.iterrows():
            upstream_id = str(row["upstream_basin_id"])
            downstream_id = str(row["downstream_basin_id"])

            if upstream_id in self.basin_to_idx and downstream_id in self.basin_to_idx:
                upstream_idx = self.basin_to_idx[upstream_id]
                downstream_idx = self.basin_to_idx[downstream_id]

                # Downstream basin should come AFTER upstream basin in the list
                if downstream_idx <= upstream_idx:
                    violations.append(
                        f"{upstream_id}(idx:{upstream_idx}) -> {downstream_id}(idx:{downstream_idx})"
                    )

        if violations:
            violation_msg = "; ".join(violations)
            LOGGER.error(
                f"Basin ordering violation detected after automatic reordering! "
                f"This should not happen. Violations: {violation_msg}"
            )
            raise RuntimeError(
                f"Internal error: Basin ordering is incorrect after automatic reordering. "
                f"Violations: {violation_msg}"
            )
        else:
            LOGGER.info(
                "Basin ordering validation passed - basins are correctly ordered upstream to downstream"
            )

    @classmethod
    def from_config(cls, cfg: Config) -> "RoutingModule":
        """Create RoutingModule from configuration."""
        return cls(cfg)

    def forward(self, lateral_flows: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        Route lateral flows through the watershed using separated flow components.

        This method now separates flows into five components:
        1. lateral_flow: Direct lateral input from catchments
        2. upstream_inflow: Flow arriving from upstream basins (routed total outflow)
        3. channel_storage: Current storage/flow in the channel
        4. channel_outflow: Flow leaving the channel (channel_storage + upstream_inflow)
        5. outflow: Total outflow of the basin (channel_outflow + lateral_flow)

        Flow routing logic (three-step process):
        1. Calculate channel outflow = channel_storage + upstream_inflow
        2. Calculate total basin outflow = channel_outflow + lateral_flow
        3. Route total outflow to downstream basins as upstream_inflow
        - At outlet basins, all flow components accumulate in channel storage

        Parameters
        ----------
        lateral_flows : torch.Tensor
            Lateral flows from LSTM models
            Shape: [batch_size, seq_len, num_basins, 1] or [batch_size, seq_len, num_basins]

        Returns
        -------
        Dict[str, torch.Tensor]
            Routing results including routed flows and outlet flow
        """
        # Handle input dimensions
        if lateral_flows.dim() == 4:
            lateral_flows = lateral_flows.squeeze(-1)  # Remove feature dimension
        elif lateral_flows.dim() != 3:
            raise ValueError(
                f"Expected lateral_flows to have 3 or 4 dimensions, got {lateral_flows.dim()}"
            )

        batch_size, seq_len, num_basins = lateral_flows.shape

        if num_basins != self.num_basins:
            raise ValueError(
                f"Input has {num_basins} basins, expected {self.num_basins}"
            )

        # Initialize flow components
        lateral_flow = lateral_flows.clone()  # Direct lateral input from catchments
        upstream_inflow = torch.zeros_like(
            lateral_flows
        )  # Flow from upstream (channel + lateral)
        channel_storage = torch.zeros_like(
            lateral_flows
        )  # Current channel storage/flow
        outflow = torch.zeros_like(lateral_flows)  # total outflow leaving each basin
        channel_outflow = torch.zeros_like(
            lateral_flows
        )  # Channel flow leaving each basin

        # Apply routing method - Process basins sequentially from upstream to downstream
        # This follows Raven's approach and ensures upstream flows are routed before downstream processing
        # Vectorized across batch and time dimensions, sequential across basins

        # Pre-compute downstream indices for each basin to avoid repeated argmax calls
        downstream_indices = torch.argmax(self.routing_matrix, dim=1)  # [num_basins]

        # Process each basin in sequence (upstream to downstream order)
        # Only the basin loop remains - batch and time dimensions are vectorized
        for basin_idx in range(self.num_basins):
            # Vectorized operations across all batches and time steps
            # Shape: [batch_size, seq_len] for all operations

            # Get flow components for this basin (vectorized)
            current_channel_storage = channel_storage[
                :, :, basin_idx
            ]  # [batch_size, seq_len]
            current_upstream_inflow = upstream_inflow[
                :, :, basin_idx
            ]  # [batch_size, seq_len]
            current_lateral_flow = lateral_flow[
                :, :, basin_idx
            ]  # [batch_size, seq_len]

            # Find downstream basin index (pre-computed)
            downstream_idx = downstream_indices[basin_idx].item()

            # 1. Calculate the total outflow from the channel using routing method (vectorized)
            channel_outflow[:, :, basin_idx] = self.route_flow(
                current_channel_storage, current_upstream_inflow
            )

            # 2. Calculate the total outflow of the basin (vectorized)
            outflow[:, :, basin_idx] = (
                channel_outflow[:, :, basin_idx] + current_lateral_flow
            )

            # Update channel storage (vectorized)
            channel_storage[:, :, basin_idx] = (
                current_channel_storage
                + current_upstream_inflow
                - channel_outflow[:, :, basin_idx]
            )

            # 3. Assign the outflow to downstream basins (if they exist) (vectorized)
            downstream_weight = self.routing_matrix[basin_idx, downstream_idx]
            if downstream_weight > 0:
                upstream_inflow[:, :, downstream_idx] += outflow[:, :, basin_idx]

        routed_flows = outflow

        # Extract outlet flow
        outlet_flow = routed_flows[
            :, :, self.outlet_idx : self.outlet_idx + 1
        ]  # [batch_size, seq_len, 1]

        return {
            "routed_flows": routed_flows,
            "outlet_flow": outlet_flow,
            "routing_matrix": self.routing_matrix,
            "lateral_flow": lateral_flow,
            "upstream_inflow": upstream_inflow,
            "channel_storage": channel_storage,
            "channel_outflow": channel_outflow,
        }
